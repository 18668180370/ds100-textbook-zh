# 四、数据清洗

数据以多种格式出现，并且在分析的实用性方面差别很大。尽管我们希望，我们所有的数据都以表格的形式出现，并且每个数值的记录都一致和准确，但实际上，我们必须仔细检查数据，找出最终可能导致错误结论的潜在问题。

术语“数据清晰”是指梳理数据，并决定如何解决不一致和缺失值的过程。我们将讨论数据集中发现的常见问题，以及解决这些问题的方法。

数据清理存在固有的局限性。例如，没有任何数据清理能够解决带偏差的采样过程。在着手进行有时很长的数据清洗过程之前，我们必须保证，我们的数据是准确收集的，尽可能没有偏差。只有这样，我们才能调查数据本身，并使用数据清洗来解决数据格式或输入过程中的问题。

我们将通过处理伯克利市警察数据集，介绍数据清洗技术。

## 调查伯克利警察数据

我们将使用伯克利警察局的公开数据集，来演示数据清洗技术。 我们已经下载了服务呼叫数据集和截停数据集。

我们可以使用带有`-lh`标志的`ls` shell 命令，来查看这些文件的更多详细信息：

```
!ls -lh data/

total 13936
-rw-r--r--@ 1 sam  staff   979K Aug 29 14:41 Berkeley_PD_-_Calls_for_Service.csv
-rw-r--r--@ 1 sam  staff    81B Aug 29 14:28 cvdow.csv
-rw-r--r--@ 1 sam  staff   5.8M Aug 29 14:41 stops.json
```

上面的命令显示了数据文件及其文件大小。 这是特别有用的，因为我们现在知道这些文件足够小，可以加载到内存中。 作为一个经验法则，将文件加载到内存中，内存大约占计算机总内存容量的四分之一，通常是安全的。 例如，如果一台电脑有 4GB 的 RAM ，我们应该可以在`pandas`中加载 1GB 的 CSV 文件。 为了处理更大的数据集，我们需要额外的计算工具，我们将在本书后面介绍。

注意在`ls`之前使用感叹号。 这告诉 Jupyter 下一行代码是 shell 命令，而不是 Python 表达式。 我们可以使用`!`在 Jupyter 中运行任何可用的 shell 命令：

```py
# The `wc` shell command shows us how many lines each file has.
# We can see that the `stops.json` file has the most lines (29852).
!wc -l data/*

   16497 data/Berkeley_PD_-_Calls_for_Service.csv
       8 data/cvdow.csv
   29852 data/stops.json
   46357 total
```

### 理解数据生成

在数据清理或处理之前，我们将陈述你应该向所有数据集询问的重要问题。 这些问题与数据的生成方式有关，因此数据清理通常无法解决这里出现的问题。

数据包含什么内容？ 服务呼叫数据的网站指出，该数据集描述了“过去 180 天内的犯罪事件（而非犯罪报告）”。 进一步阅读表明“并非所有警务服务的呼叫都包含在内（例如动物咬伤）”。

截停数据的网站指出，该数据集包含自 2015 年 1 月 26 日起的所有“车辆截停（包括自行车）和行人截停（最多五人）”的数据。

数据是普查吗？ 这取决于我们感兴趣的人群。 例如，如果我们感兴趣的是，过去 180 天内的犯罪事件的服务呼叫，那么呼叫数据集就是一个普查。 但是，如果我们感兴趣的是，过去 10 年内的服务呼叫，数据集显然不是普查。 由于数据收集开始于 2015 年 1 月 26 日，我们可以对截停数据集做出类似的猜测。

如果数据构成一个样本，它是概率样本吗？ 如果我们正在调查一个时间段，数据没有它的条目，那么数据不会形成概率样本，因为在数据收集过程中没有涉及随机性 - 我们有一定时间段的所有数据，但其他时间段没有数据。

这些数据对我们的结论有何限制？ 虽然我们会在数据处理的每一步都提出这个问题，但我们已经可以看到，我们的数据带有重要的限制。 最重要的限制是，我们不能对我们的数据集未涵盖的时间段进行无偏估计。

### 清洗呼叫数据集

现在我们来清理呼叫数据集。`head` shell 命令打印文件的前五行。

```
!head data/Berkeley_PD_-_Calls_for_Service.csv

CASENO,OFFENSE,EVENTDT,EVENTTM,CVLEGEND,CVDOW,InDbDate,Block_Location,BLKADDR,City,State
17091420,BURGLARY AUTO,07/23/2017 12:00:00 AM,06:00,BURGLARY - VEHICLE,0,08/29/2017 08:28:05 AM,"2500 LE CONTE AVE
Berkeley, CA
(37.876965, -122.260544)",2500 LE CONTE AVE,Berkeley,CA
17020462,THEFT FROM PERSON,04/13/2017 12:00:00 AM,08:45,LARCENY,4,08/29/2017 08:28:00 AM,"2200 SHATTUCK AVE
Berkeley, CA
(37.869363, -122.268028)",2200 SHATTUCK AVE,Berkeley,CA
17050275,BURGLARY AUTO,08/24/2017 12:00:00 AM,18:30,BURGLARY - VEHICLE,4,08/29/2017 08:28:06 AM,"200 UNIVERSITY AVE
Berkeley, CA
(37.865491, -122.310065)",200 UNIVERSITY AVE,Berkeley,CA
```

它似乎是逗号分隔值（CSV）文件，尽管很难判断整个文件是否格式正确。 我们可以使用`pd.read_csv`将文件读取为`DataFrame`。 如果`pd.read_csv`产生错误，我们将不得不更进一步并手动解决格式问题。 幸运的是，`pd.read_csv`成功返回一个`DataFrame`：

```py
calls = pd.read_csv('data/Berkeley_PD_-_Calls_for_Service.csv')
calls
```

| | CASENO | OFFENSE | EVENTDT | EVENTTM | ... | Block_Location | BLKADDR | City | State |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| 0 | 17091420 | BURGLARY AUTO | 07/23/2017 12:00:00 AM | 06:00 | ... | 2500 LE CONTE AVE\nBerkeley, CA\n(37.876965, -... | 2500 LE CONTE AVE | Berkeley | CA |
| 1 | 17020462 | THEFT FROM PERSON | 04/13/2017 12:00:00 AM | 08:45 | ... | 2200 SHATTUCK AVE\nBerkeley, CA\n(37.869363, -... | 2200 SHATTUCK AVE | Berkeley | CA |
| 2 | 17050275 | BURGLARY AUTO | 08/24/2017 12:00:00 AM | 18:30 | ... | 200 UNIVERSITY AVE\nBerkeley, CA\n(37.865491, ... | 200 UNIVERSITY AVE | Berkeley | CA |
| ... | ... | ... | ... | ... | ... | ... | ... | ... | ... |
| 5505 | 17018126 | DISTURBANCE | 04/01/2017 12:00:00 AM | 12:22 | ... | 1600 FAIRVIEW ST\nBerkeley, CA\n(37.850001, -1... | 1600 FAIRVIEW ST | Berkeley | CA |
| 5506 | 17090665 | THEFT MISD. (UNDER $950) | 04/01/2017 12:00:00 AM | 12:00 | ... | 2000 DELAWARE ST\nBerkeley, CA\n(37.874489, -1... | 2000 DELAWARE ST | Berkeley | CA |
| 5507 | 17049700 | SEXUAL ASSAULT MISD. | 08/22/2017 12:00:00 AM | 20:02 | ... | 2400 TELEGRAPH AVE\nBerkeley, CA\n(37.866761, ... | 2400 TELEGRAPH AVE | Berkeley | CA |

5508 行 × 11 列

我们可以定义一个函数来显示数据的不同片段，然后与之交互：

```py
def df_interact(df):
    '''
    Outputs sliders that show rows and columns of df
    '''
    def peek(row=0, col=0):
        return df.iloc[row:row + 5, col:col + 6]
    interact(peek, row=(0, len(df), 5), col=(0, len(df.columns) - 6))
    print('({} rows, {} columns) total'.format(df.shape[0], df.shape[1]))

df_interact(calls)
# (5508 rows, 11 columns) total
```

根据上面的输出结果，生成的`DataFrame`看起来很合理，因为列的名称正确，每列中的数据看起来都是一致的。 每列包含哪些数据？ 我们可以查看数据集网站：

| 列 | 描述 | 类型 |
| --- | --- | --- |
| CASENO | 案件编号 | 数字 |
| OFFENSE | 案件类型 | 纯文本 |
| EVENTDT | 事件的发生日期 | 日期时间 |
| EVENTTM | 事件的发生时间 | 纯文本 |
| CVLEGEND | 事件描述 | 纯文本 |
| CVDOW | 时间的发生星期 | 数字 |
| InDbDate | 数据集的上传日期 | 日期时间 |
| Block_Location | 事件的街区级别的地址 | 地点 |
| BLKADDR |  | 纯文本 |
| City |  | 纯文本 |
| State |  | 纯文本 |

数据表面上看起来很容易处理。 但是，在开始数据分析之前，我们必须回答以下问题：

数据集中是否存在缺失值？ 这个问题很重要，因为缺失值可能代表许多不同的事情。 例如，遗漏的地址可能意味着删除了地点来保护隐私，或者某些受访者选择不回答调查问题，或录制设备损坏。
是否有已填写的缺失值（例如 999 岁，未知年龄，或上午 12:00 为未知日期）？ 如果我们忽略它们，它们显然将影响分析。
数据的哪些部分是由人类输入的？ 我们将很快看到，人类输入的数据充满了不一致和错误拼写。
虽然要通过更多检查，但这三种检查方法在很多情况下都足够了。 查看 [Quartz  的不良数据指南](https://github.com/Quartz/bad-data-guide)，来获取更完整的检查列表。

### 是否存在缺失值？

`pandas`中这是一个简单的检查：

```py
# True if row contains at least one null value
null_rows = calls.isnull().any(axis=1)
calls[null_rows]
```

| | CASENO | OFFENSE | EVENTDT | EVENTTM | ... | Block_Location | BLKADDR | City | State |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| 116 | 17014831 | BURGLARY AUTO | 03/16/2017 12:00:00 AM | 22:00 | ... | Berkeley, CA\n(37.869058, -122.270455) | NaN | Berkeley | CA |
| 478 | 17042511 | BURGLARY AUTO | 07/20/2017 12:00:00 AM | 16:00 | ... | Berkeley, CA\n(37.869058, -122.270455) | NaN | Berkeley | CA |
| 486 | 17022572 | VEHICLE STOLEN | 04/22/2017 12:00:00 AM | 21:00 | ... | Berkeley, CA\n(37.869058, -122.270455) | NaN | Berkeley | CA |
| ... | ... | ... | ... | ... | ... | ... | ... | ... | ... |
| 4945 | 17091287 | VANDALISM | 07/01/2017 12:00:00 AM | 08:00 | ... | Berkeley, CA\n(37.869058, -122.270455) | NaN | Berkeley | CA |
| 4947 | 17038382 | BURGLARY RESIDENTIAL | 06/30/2017 12:00:00 AM | 15:00 | ... | Berkeley, CA\n(37.869058, -122.270455) | NaN | Berkeley | CA |
| 5167 | 17091632 | VANDALISM | 08/15/2017 12:00:00 AM | 23:30 | ... | Berkeley, CA\n(37.869058, -122.270455) | NaN | Berkeley | CA |

27 行 × 11 列

看起来`BLKADDR`中有 27 个呼叫没有地址记录。 不幸的是，对于地点的记录方式，数据描述并不十分清楚。 我们知道，所有这些呼叫都是由于伯克利的事件，因此我们可以认为，这些呼叫的地址最初是在伯克利的某个地方。

### 有没有已填充的缺失值？

从上面的缺失值检查中，我们可以看到，如果位置缺失，`Block_Location`列会记录`Berkeley, CA`。

另外，通过查看呼叫表，我们发现`EVENTDT`列日期正确，但所有时间都记录了上午 12 点。 相反，时间在`EVENTTM`列中。

```py
# Show the first 7 rows of the table again for reference
calls.head(7)
```

| | CASENO | OFFENSE | EVENTDT | EVENTTM | ... | Block_Location | BLKADDR | City | State |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| 0 | 17091420 | BURGLARY AUTO | 07/23/2017 12:00:00 AM | 06:00 | ... | 2500 LE CONTE AVE\nBerkeley, CA\n(37.876965, -... | 2500 LE CONTE AVE | Berkeley | CA |
| 1 | 17020462 | THEFT FROM PERSON | 04/13/2017 12:00:00 AM | 08:45 | ... | 2200 SHATTUCK AVE\nBerkeley, CA\n(37.869363, -... | 2200 SHATTUCK AVE | Berkeley | CA |
| 2 | 17050275 | BURGLARY AUTO | 08/24/2017 12:00:00 AM | 18:30 | ... | 200 UNIVERSITY AVE\nBerkeley, CA\n(37.865491, ... | 200 UNIVERSITY AVE | Berkeley | CA |
| 3 | 17019145 | GUN/WEAPON | 04/06/2017 12:00:00 AM | 17:30 | ... | 1900 SEVENTH ST\nBerkeley, CA\n(37.869318, -12... | 1900 SEVENTH ST | Berkeley | CA |
| 4 | 17044993 | VEHICLE STOLEN | 08/01/2017 12:00:00 AM | 18:00 | ... | 100 PARKSIDE DR\nBerkeley, CA\n(37.854247, -12... | 100 PARKSIDE DR | Berkeley | CA |
| 5 | 17037319 | BURGLARY RESIDENTIAL | 06/28/2017 12:00:00 AM | 12:00 | ... | 1500 PRINCE ST\nBerkeley, CA\n(37.851503, -122... | 1500 PRINCE ST | Berkeley | CA |
| 6 | 17030791 | BURGLARY RESIDENTIAL | 05/30/2017 12:00:00 AM | 08:45 | ... | 300 MENLO PL\nBerkeley, CA\n | 300 MENLO PL | Berkeley | CA |

7 行 × 11 列

作为数据清理步骤，我们希望合并`EVENTDT`和`EVENTTM`列，在一个字段中记录日期和时间。 如果我们定义一个函数，接受`DF`并返回新的`DF`，我们可以稍后使用`pd.pipe`一次性应用所有转换。

```py
def combine_event_datetimes(calls):
    combined = pd.to_datetime(
        # Combine date and time strings
        calls['EVENTDT'].str[:10] + ' ' + calls['EVENTTM'],
        infer_datetime_format=True,
    )
    return calls.assign(EVENTDTTM=combined)

# To peek at the result without mutating the calls DF:
calls.pipe(combine_event_datetimes).head(2)
```

| | CASENO | OFFENSE | EVENTDT | EVENTTM | ... | BLKADDR | City | State | EVENTDTTM |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| 0 | 17091420 | BURGLARY AUTO | 07/23/2017 12:00:00 AM | 06:00 | ... | 2500 LE CONTE AVE | Berkeley | CA | 2017-07-23 06:00:00 |
| 1 | 17020462 | THEFT FROM PERSON | 04/13/2017 12:00:00 AM | 08:45 | ... | 2200 SHATTUCK AVE | Berkeley | CA | 2017-04-13 08:45:00 |

2 行 × 12 列

### 数据的哪些部分是由人类输入的？

看起来，大多数数据列是机器记录的，包括日期，时间，星期和事件位置。

另外，`OFFENSE`和`CVLEGEND`列看起来包含一致的值。 我们可以检查每列中的唯一值，来查看是否有任何拼写错误：

```py
calls['OFFENSE'].unique()
'''
array(['BURGLARY AUTO', 'THEFT FROM PERSON', 'GUN/WEAPON',
       'VEHICLE STOLEN', 'BURGLARY RESIDENTIAL', 'VANDALISM',
       'DISTURBANCE', 'THEFT MISD. (UNDER $950)', 'THEFT FROM AUTO',
       'DOMESTIC VIOLENCE', 'THEFT FELONY (OVER $950)', 'ALCOHOL OFFENSE',
       'MISSING JUVENILE', 'ROBBERY', 'IDENTITY THEFT',
       'ASSAULT/BATTERY MISD.', '2ND RESPONSE', 'BRANDISHING',
       'MISSING ADULT', 'NARCOTICS', 'FRAUD/FORGERY',
       'ASSAULT/BATTERY FEL.', 'BURGLARY COMMERCIAL', 'MUNICIPAL CODE',
       'ARSON', 'SEXUAL ASSAULT FEL.', 'VEHICLE RECOVERED',
       'SEXUAL ASSAULT MISD.', 'KIDNAPPING', 'VICE', 'HOMICIDE'], dtype=object)
'''
```

```py
calls['CVLEGEND'].unique()
'''
array(['BURGLARY - VEHICLE', 'LARCENY', 'WEAPONS OFFENSE',
       'MOTOR VEHICLE THEFT', 'BURGLARY - RESIDENTIAL', 'VANDALISM',
       'DISORDERLY CONDUCT', 'LARCENY - FROM VEHICLE', 'FAMILY OFFENSE',
       'LIQUOR LAW VIOLATION', 'MISSING PERSON', 'ROBBERY', 'FRAUD',
       'ASSAULT', 'NOISE VIOLATION', 'DRUG VIOLATION',
       'BURGLARY - COMMERCIAL', 'ALL OTHER OFFENSES', 'ARSON', 'SEX CRIME',
       'RECOVERED VEHICLE', 'KIDNAPPING', 'HOMICIDE'], dtype=object)
'''
```

由于这些列中的每个值似乎都拼写正确，因此我们不必对这些列执行任何更正。

我们还检查了`BLKADDR`列的不一致性，发现有时记录了地址（例如`2500LE CONTE AVE`），但有时记录十字路口（例如`ALLSTON WAY & FIFTH ST`）。 这表明人类输入了这些数据，而这一栏很难用于分析。 幸运的是，我们可以使用事件的经纬度而不是街道地址。

```py
calls['BLKADDR'][[0, 5001]]
'''
0            2500 LE CONTE AVE
5001    ALLSTON WAY & FIFTH ST
Name: BLKADDR, dtype: object
'''
```

### 最后的接触

这个数据集似乎几乎可用于分析。 `Block_Location`列似乎包含记录地址，纬度和经度的字符串。 我们将要分割经纬度以便使用。

```py
def split_lat_lon(calls):
    return calls.join(
        calls['Block_Location']
        # Get coords from string
        .str.split('\n').str[2]
        # Remove parens from coords
        .str[1:-1]
        # Split latitude and longitude
        .str.split(', ', expand=True)
        .rename(columns={0: 'Latitude', 1: 'Longitude'})
    )

calls.pipe(split_lat_lon).head(2)
```

| | CASENO | OFFENSE | EVENTDT | EVENTTM | ... | City | State | Latitude | Longitude |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| 0 | 17091420 | BURGLARY AUTO | 07/23/2017 12:00:00 AM | 06:00 | ... | Berkeley | CA | 37.876965 | -122.260544 |
| 1 | 17020462 | THEFT FROM PERSON | 04/13/2017 12:00:00 AM | 08:45 | ... | Berkeley | CA | 37.869363 | -122.268028 |

2 行 × 13 列

然后，我们可以将星期序号与星期进行匹配：

```py
# This DF contains the day for each number in CVDOW
day_of_week = pd.read_csv('data/cvdow.csv')
day_of_week
```

| | CVDOW | Day |
| --- | --- | --- |
| 0 | 0 | Sunday |
| 1 | 1 | Monday |
| 2 | 2 | Tuesday |
| 3 | 3 | Wednesday |
| 4 | 4 | Thursday |
| 5 | 5 | Friday |
| 6 | 6 | Saturday |

```py
def match_weekday(calls):
    return calls.merge(day_of_week, on='CVDOW')
calls.pipe(match_weekday).head(2)
```

| | CASENO | OFFENSE | EVENTDT | EVENTTM | ... | BLKADDR | City | State | Day |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| 0 | 17091420 | BURGLARY AUTO | 07/23/2017 12:00:00 AM | 06:00 | ... | 2500 LE CONTE AVE | Berkeley | CA | Sunday |
| 1 | 17038302 | BURGLARY AUTO | 07/02/2017 12:00:00 AM | 22:00 | ... | BOWDITCH STREET & CHANNING WAY | Berkeley | CA | Sunday |

2 行 × 12 列

我们将删除我们不再需要的列：

```py
def drop_unneeded_cols(calls):
    return calls.drop(columns=['CVDOW', 'InDbDate', 'Block_Location', 'City',
                               'State', 'EVENTDT', 'EVENTTM'])
```

最后，我们让`calls` DF 穿过我们定义的所有函数的管道：

```py
calls_final = (calls.pipe(combine_event_datetimes)
               .pipe(split_lat_lon)
               .pipe(match_weekday)
               .pipe(drop_unneeded_cols))
df_interact(calls_final)
```

户籍数据集现在可用于进一步的数据分析。 在下一节中，我们将清理截停数据集。

```py
# HIDDEN
# Save data to CSV for other chapters
# calls_final.to_csv('../ch5/data/calls.csv', index=False)
```
