# 四、数据清洗

数据以多种格式出现，并且在分析的实用性方面差别很大。尽管我们希望，我们所有的数据都以表格的形式出现，并且每个数值的记录都一致和准确，但实际上，我们必须仔细检查数据，找出最终可能导致错误结论的潜在问题。

术语“数据清晰”是指梳理数据，并决定如何解决不一致和缺失值的过程。我们将讨论数据集中发现的常见问题，以及解决这些问题的方法。

数据清理存在固有的局限性。例如，没有任何数据清理能够解决带偏差的采样过程。在着手进行有时很长的数据清洗过程之前，我们必须保证，我们的数据是准确收集的，尽可能没有偏差。只有这样，我们才能调查数据本身，并使用数据清洗来解决数据格式或输入过程中的问题。

我们将通过处理伯克利市警察数据集，介绍数据清洗技术。

## 调查伯克利警察数据

我们将使用伯克利警察局的公开数据集，来演示数据清洗技术。 我们已经下载了服务呼叫数据集和滞留数据集。

我们可以使用带有`-lh`标志的`ls` shell 命令，来查看这些文件的更多详细信息：

```
!ls -lh data/
```

上面的命令显示了数据文件及其文件大小。 这是特别有用的，因为我们现在知道这些文件足够小，可以加载到内存中。 作为一个经验法则，将文件加载到内存中，内存大约占计算机总内存容量的四分之一，通常是安全的。 例如，如果一台电脑有 4GB 的 RAM ，我们应该可以在`pandas`中加载 1GB 的 CSV 文件。 为了处理更大的数据集，我们需要额外的计算工具，我们将在本书后面介绍。

注意在`ls`之前使用感叹号。 这告诉 Jupyter 下一行代码是 shell 命令，而不是 Python 表达式。 我们可以使用`!`在 Jupyter 中运行任何可用的 shell 命令：

```py
# The `wc` shell command shows us how many lines each file has.
# We can see that the `stops.json` file has the most lines (29852).
!wc -l data/*
```

### 理解数据生成

在数据清理或处理之前，我们将陈述你应该向所有数据集询问的重要问题。 这些问题与数据的生成方式有关，因此数据清理通常无法解决这里出现的问题。

数据包含什么内容？ 服务呼叫数据的网站指出，该数据集描述了“过去 180 天内的犯罪事件（而非犯罪报告）”。 进一步阅读表明“并非所有警务服务的呼叫都包含在内（例如动物咬伤）”。

滞留数据的网站指出，该数据集包含自 2015 年 1 月 26 日起的所有“车辆滞留（包括自行车）和行人滞留（最多五人）”的数据。

数据是普查吗？ 这取决于我们感兴趣的人群。 例如，如果我们感兴趣的是，过去 180 天内的犯罪事件的服务呼叫，那么呼叫数据集就是一个普查。 但是，如果我们感兴趣的是，过去 10 年内的服务呼叫，数据集显然不是普查。 由于数据收集开始于 2015 年 1 月 26 日，我们可以对滞留数据集做出类似的猜测。

如果数据构成一个样本，它是概率样本吗？ 如果我们正在调查一个时间段，数据没有它的条目，那么数据不会形成概率样本，因为在数据收集过程中没有涉及随机性 - 我们有一定时间段的所有数据，但其他时间段没有数据。

这些数据对我们的结论有何限制？ 虽然我们会在数据处理的每一步都提出这个问题，但我们已经可以看到，我们的数据带有重要的限制。 最重要的限制是，我们不能对我们的数据集未涵盖的时间段进行无偏估计。

### 清洗呼叫数据集

现在我们来清理呼叫数据集。`head` shell 命令打印文件的前五行。

```
!head data/Berkeley_PD_-_Calls_for_Service.csv
```

它似乎是逗号分隔值（CSV）文件，尽管很难判断整个文件是否格式正确。 我们可以使用`pd.read_csv`将文件读取为`DataFrame`。 如果`pd.read_csv`产生错误，我们将不得不更进一步并手动解决格式问题。 幸运的是，`pd.read_csv`成功返回一个`DataFrame`：

```py
calls = pd.read_csv('data/Berkeley_PD_-_Calls_for_Service.csv')
calls
```

我们可以定义一个函数来显示数据的不同片段，然后与之交互：

```py
def df_interact(df):
    '''
    Outputs sliders that show rows and columns of df
    '''
    def peek(row=0, col=0):
        return df.iloc[row:row + 5, col:col + 6]
    interact(peek, row=(0, len(df), 5), col=(0, len(df.columns) - 6))
    print('({} rows, {} columns) total'.format(df.shape[0], df.shape[1]))

df_interact(calls)
```

根据上面的输出结果，生成的`DataFrame`看起来很合理，因为列的名称正确，每列中的数据看起来都是一致的。 每列包含哪些数据？ 我们可以查看数据集网站：

| 列 | 描述 | 类型 |
| --- | --- | --- |
| CASENO | 案件编号 | 数字 |
| OFFENSE | 案件类型 | 纯文本 |
| EVENTDT | 事件的发生日期 | 日期时间 |
| EVENTTM | 事件的发生时间 | 纯文本 |
| CVLEGEND | 事件描述 | 纯文本 |
| CVDOW | 时间的发生星期 | 数字 |
| InDbDate | 数据集的上传日期 | 日期时间 |
| Block_Location | 事件的街区级别的地址 | 地点 |
| BLKADDR |  | 纯文本 |
| City |  | 纯文本 |
| State |  | 纯文本 |

数据表面上看起来很容易处理。 但是，在开始数据分析之前，我们必须回答以下问题：

数据集中是否存在缺失值？ 这个问题很重要，因为缺失值可能代表许多不同的事情。 例如，遗漏的地址可能意味着删除了地点来保护隐私，或者某些受访者选择不回答调查问题，或录制设备损坏。
是否有已填写的缺失值（例如 999 岁，未知年龄，或上午 12:00 为未知日期）？ 如果我们忽略它们，它们显然将影响分析。
数据的哪些部分是由人类输入的？ 我们将很快看到，人类输入的数据充满了不一致和错误拼写。
虽然要通过更多检查，但这三种检查方法在很多情况下都足够了。 查看 [Quartz  的不良数据指南](https://github.com/Quartz/bad-data-guide)，来获取更完整的检查列表。

### 是否存在缺失值？

`pandas`中这是一个简单的检查：

```py
# True if row contains at least one null value
null_rows = calls.isnull().any(axis=1)
calls[null_rows]
```

看起来`BLKADDR`中有 27 个呼叫没有地址记录。 不幸的是，对于地点的记录方式，数据描述并不十分清楚。 我们知道，所有这些呼叫都是由于伯克利的事件，因此我们可以认为，这些呼叫的地址最初是在伯克利的某个地方。

### 有没有已填充的缺失值？

从上面的缺失值检查中，我们可以看到，如果位置缺失，`Block_Location`列会记录`Berkeley, CA`。

另外，通过查看呼叫表，我们发现`EVENTDT`列日期正确，但所有时间都记录了上午 12 点。 相反，时间在`EVENTTM`列中。

```py
# Show the first 7 rows of the table again for reference
calls.head(7)
```

作为数据清理步骤，我们希望合并`EVENTDT`和`EVENTTM`列，在一个字段中记录日期和时间。 如果我们定义一个函数，接受`DF`并返回新的`DF`，我们可以稍后使用`pd.pipe`一次性应用所有转换。

```py
def combine_event_datetimes(calls):
    combined = pd.to_datetime(
        # Combine date and time strings
        calls['EVENTDT'].str[:10] + ' ' + calls['EVENTTM'],
        infer_datetime_format=True,
    )
    return calls.assign(EVENTDTTM=combined)

# To peek at the result without mutating the calls DF:
calls.pipe(combine_event_datetimes).head(2)
```

### 数据的哪些部分是由人类输入的？

看起来，大多数数据列是机器记录的，包括日期，时间，星期和事件位置。

另外，`OFFENSE`和`CVLEGEND`列看起来包含一致的值。 我们可以检查每列中的唯一值，来查看是否有任何拼写错误：

```py
calls['OFFENSE'].unique()
```

```py
calls['CVLEGEND'].unique()
```

由于这些列中的每个值似乎都拼写正确，因此我们不必对这些列执行任何更正。

我们还检查了`BLKADDR`列的不一致性，发现有时记录了地址（例如`2500LE CONTE AVE`），但有时记录十字路口（例如`ALLSTON WAY & FIFTH ST`）。 这表明人类输入了这些数据，而这一栏很难用于分析。 幸运的是，我们可以使用事件的经纬度而不是街道地址。

```py
calls['BLKADDR'][[0, 5001]]
```

### 最后的接触

这个数据集似乎几乎可用于分析。 `Block_Location`列似乎包含记录地址，纬度和经度的字符串。 我们将要分割经纬度以便使用。

```py
def split_lat_lon(calls):
    return calls.join(
        calls['Block_Location']
        # Get coords from string
        .str.split('\n').str[2]
        # Remove parens from coords
        .str[1:-1]
        # Split latitude and longitude
        .str.split(', ', expand=True)
        .rename(columns={0: 'Latitude', 1: 'Longitude'})
    )

calls.pipe(split_lat_lon).head(2)
```

然后，我们可以将星期序号与星期进行匹配：

```py
# This DF contains the day for each number in CVDOW
day_of_week = pd.read_csv('data/cvdow.csv')
day_of_week
```

```py
def match_weekday(calls):
    return calls.merge(day_of_week, on='CVDOW')
calls.pipe(match_weekday).head(2)
```

我们将删除我们不再需要的列：

```py
def drop_unneeded_cols(calls):
    return calls.drop(columns=['CVDOW', 'InDbDate', 'Block_Location', 'City',
                               'State', 'EVENTDT', 'EVENTTM'])
```

最后，我们让`calls` DF 穿过我们定义的所有函数的管道：

```py
calls_final = (calls.pipe(combine_event_datetimes)
               .pipe(split_lat_lon)
               .pipe(match_weekday)
               .pipe(drop_unneeded_cols))
df_interact(calls_final)
```

户籍数据集现在可用于进一步的数据分析。 在下一节中，我们将清理滞留数据集。

```py
# HIDDEN
# Save data to CSV for other chapters
# calls_final.to_csv('../ch5/data/calls.csv', index=False)
```
